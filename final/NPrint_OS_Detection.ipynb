{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-11T22:04:39.961778Z",
     "start_time": "2025-12-11T22:04:39.957791Z"
    }
   },
   "cell_type": "markdown",
   "source": "In this NPrint OS Detection project, we will apply machine learning to fingerprint operating systems based on their network traffic patterns. The dataset contains traffic from 13 classes of operating systems, and our first task is to extract the labels from the pcapng file. Labels are stored in the packet comments, and I used tshark for this task. Please update the path below to your local installation.",
   "id": "651938e00fc8e484"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T22:53:24.460616Z",
     "start_time": "2025-12-11T22:53:24.457363Z"
    }
   },
   "cell_type": "code",
   "source": "TSHARK = r\"C:\\Program Files\\Wireshark\\tshark.exe\"",
   "id": "d0fb4ba1040c5c6e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we will later see, we need a mechanism to match the extracted OS labels to the corresponding packets. I used a dictionary of timestamps to OS labels, where it is assumed that the timestamp will be unique for each packet.",
   "id": "86e46ab8e45b1de6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T22:53:24.470073Z",
     "start_time": "2025-12-11T22:53:24.465618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "def extract_os_labels(pcap_file):\n",
    "    \"\"\"\n",
    "    Extract OS labels from pcapng packet comments using tshark.\n",
    "    Returns a dictionary mapping normalized timestamps of packets to OS labels.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(TSHARK):\n",
    "        print(\"tshark not found\")\n",
    "        return {}\n",
    "\n",
    "    ts_to_label = OrderedDict()\n",
    "    pending_label = None\n",
    "\n",
    "    cmd = [TSHARK, \"-r\", pcap_file, \"-V\"]\n",
    "    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    for line in proc.stdout:\n",
    "        if \"Packet comments\" in line:\n",
    "            raw = next(proc.stdout, \"\").strip()\n",
    "            if \",\" in raw and \"_\" in raw:\n",
    "                _, pair = raw.split(\",\", 1)\n",
    "                _, hard = pair.split(\"_\", 1)\n",
    "                pending_label = hard.strip()\n",
    "            else:\n",
    "                pending_label = None\n",
    "            continue\n",
    "\n",
    "        if pending_label is not None:\n",
    "            m_ts = re.search(r\"Epoch Arrival Time:\\s*([0-9]+\\.[0-9]+)\", line)\n",
    "            if m_ts:\n",
    "                ts = m_ts.group(1).rstrip('0').rstrip('.')\n",
    "                ts_to_label[ts] = pending_label\n",
    "                pending_label = None\n",
    "\n",
    "    proc.wait()\n",
    "    return ts_to_label"
   ],
   "id": "731b3586ca4f761b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now onto data processing. We will use netml to convert the packets into flows. The two features we will use are IAT and STATS. Different operating systems implement TCP/IP protocols with different IAT characteristics due to variations in congestion control algorithms. STATS captures structural protocol differences, offering complementary features to IAT's temporal fingerprints.",
   "id": "efbeacfe2224a62a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T22:53:25.210156Z",
     "start_time": "2025-12-11T22:53:24.475076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from netml.pparser.parser import PCAP\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(pcap_file):\n",
    "    \"\"\"\n",
    "    Extract IAT and STATS features from a pcap file using netml.\n",
    "    Returns combined feature array and list of flows.\n",
    "    \"\"\"\n",
    "    pcap = PCAP(pcap_file)\n",
    "    pcap.pcap2flows()\n",
    "\n",
    "    pcap.flow2features('IAT', fft=False, header=False)\n",
    "    iat_features = pcap.features.copy()\n",
    "\n",
    "    pcap.flow2features('STATS', fft=False, header=False)\n",
    "    stats_features = pcap.features\n",
    "\n",
    "    combined = np.hstack([iat_features, stats_features])\n",
    "    return combined, pcap.flows"
   ],
   "id": "c1036cfc0017e39f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Wireshark is installed, but cannot read manuf !\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T22:13:39.538574Z",
     "start_time": "2025-12-11T22:13:39.536051Z"
    }
   },
   "cell_type": "markdown",
   "source": "The flows above have no knowledge of the operating system labels that we extracted. We will take the first packet of each flow and use its timestamp to search through the labels dictionary. Recall that the packets of each flow are guaranteed to come from the same source ip. However, the same ip may map to multiple operating systems in this dataset. We lose accuracy assuming the operating system of the first packet as the entire flow's label, but considering that the dataset contains millions of packets, we cannot afford to train on individual packets. The compromise of accuracy using flows is required.",
   "id": "1708102ea3104688"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T22:53:25.217661Z",
     "start_time": "2025-12-11T22:53:25.214157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "def build_dataset(features, flows, ts_to_label):\n",
    "    \"\"\"\n",
    "    Build labeled dataset by matching flow timestamps to OS labels.\n",
    "    Returns feature matrix X and label array y.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(features)\n",
    "\n",
    "    labels = []\n",
    "    for flow_key, packets in flows:\n",
    "        ts = str(packets[0].time).rstrip('0').rstrip('.')\n",
    "        labels.append(ts_to_label.get(ts))\n",
    "\n",
    "    df['label'] = labels\n",
    "    df = df.dropna(subset=['label'])\n",
    "\n",
    "    X = df.drop(columns=['label']).values\n",
    "    y = df['label'].values\n",
    "    return X, y"
   ],
   "id": "efd163b6d6760a24",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now onto the training and evaluation. We will test four different models in this project. Ridge and Logistic are linear models that provide simple, fast baselines. Neural Network is a naive attempt to capture non-linear relationships. Lastly, Random Forest provides interpretable thresholds, allowing us to understand which specific protocol characteristics distinguish operating systems, unlike neural networks which operate as black boxes.",
   "id": "2b11a8667881e406"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T22:53:26.188835Z",
     "start_time": "2025-12-11T22:53:25.221168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "def train_and_evaluate(X, y):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple classifiers using balanced accuracy.\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    models = {\n",
    "        \"Ridge\": (RidgeClassifier(alpha=1.0, random_state=42), True),\n",
    "        \"Logistic\": (LogisticRegression(max_iter=500, random_state=42), True),\n",
    "        \"NeuralNet\": (MLPClassifier(hidden_layer_sizes=(50,), max_iter=500, random_state=42), True),\n",
    "        \"RandomForest\": (RandomForestClassifier(n_estimators=100, random_state=42), False),\n",
    "    }\n",
    "\n",
    "    for name, (model, needs_scaling) in models.items():\n",
    "        if needs_scaling:\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            preds = model.predict(X_test_scaled)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            preds = model.predict(X_test)\n",
    "        acc = balanced_accuracy_score(y_test, preds)\n",
    "        print(f\"{name:12} Balanced Accuracy: {acc:.4f}\")"
   ],
   "id": "a60da9d8ad0dc86",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We are now ready to feed our pipeline raw data. Note that feeding the complete os-100-packets.pcapng file took 15 minutes on my machine. The results are discussed in the write-up. The dataset can be found at https://nprint.github.io/benchmarks/os_detection/nprint_os_detection.html",
   "id": "afc120daadfc9ee9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T23:09:50.355707Z",
     "start_time": "2025-12-11T22:53:26.255213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pcap_file = \"os-100-packets.pcapng\"\n",
    "\n",
    "ts_to_label = extract_os_labels(pcap_file)\n",
    "\n",
    "features, flows = extract_features(pcap_file)\n",
    "\n",
    "X, y = build_dataset(features, flows, ts_to_label)\n",
    "train_and_evaluate(X, y)"
   ],
   "id": "4db991bcfe629171",
   "outputs": [
    
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge        Balanced Accuracy: 0.2380\n",
      "Logistic     Balanced Accuracy: 0.2875\n",
      "NeuralNet    Balanced Accuracy: 0.4448\n",
      "RandomForest Balanced Accuracy: 0.6557\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
